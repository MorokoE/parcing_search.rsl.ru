# -*- coding: utf-8 -*-
"""парсинг библиотеки РГБ.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EK0JOKVYibbo1_4YsCPrHngX38BCnGHL

# Парсинг диссертаций с search.rsl.ru
"""

#Импортируем все библиотеки, которые нам понадобятся для парсинга диссертаций и экспорта их впоследствии в формате csv 
import pandas as pd
import re
import requests
from bs4 import BeautifulSoup
from time import sleep
from random import randint
import datetime 
import time
import numpy as np

! pip install selenium #библиотека, которая понадобится для имитации пользовательского поведения в браузере

from selenium import webdriver as wb

!pip install selenium
!apt-get update 
!apt install chromium-chromedriver

from selenium import webdriver
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')
wd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)
driver =webdriver.Chrome('chromedriver',chrome_options=chrome_options)

diss_data_merged= pd.DataFrame(columns= ["name", "date_submitted", 'description'])

for i in range(2, 101):
    br = webdriver.Chrome('chromedriver',chrome_options=chrome_options) 
    br.get('https://search.rsl.ru/#yf=2010&yt=2010&s=default&vc=22.00.00&vc=23.00.00&vc=24.00.00&vc=17.00.00&p='+str(i)+'&d=xdis') #Указываем сайт, с которого будем парсить данные
    sleep(3)
    soup = BeautifulSoup(br.page_source)
    br.quit()
    name = soup.findAll('div', attrs={'class':'rsl-item-nocover rsl-item-nocover-title'}) #Определяем тэги HTML-кода сайта, в которых лежит необходимая для нас информация
    date = soup.findAll('p', attrs={'class':'search-item-enterdate'})
    info = soup.find_all('span', attrs={'class': 'js-item-maininfo'})

    name_clean = [i.get_text() for i in name] #Достаем из контейнеров имя, дату и дополнительные сведения о каждой диссертации
    date_clean = [i.get_text() for i in date]
    info_clean = [i.get_text() for i in info]

    diss_dict = {"name": name_clean, "date_submitted": date_clean, 'description': info_clean}
    diss_data = pd.DataFrame(diss_dict)
    diss_data_merged = diss_data_merged.append(diss_data)
    diss_data_merged = pd.DataFrame(diss_data_merged)
    diss_data_merged.to_csv('b1.csv', index=False) #записываем полученные сведения в csv
    if len(diss_data_merged) % 500 == 0:
      print(len(diss_data_merged))

for i in range(2, 101):
    br = webdriver.Chrome('chromedriver',chrome_options=chrome_options) 
    br.get('https://search.rsl.ru/#yf=2011&yt=2011&s=default&vc=22.00.00&vc=23.00.00&vc=24.00.00&vc=17.00.00&p='+str(i)+'&d=xdis')
    sleep(3)
    soup = BeautifulSoup(br.page_source)
    br.quit()
    name = soup.findAll('div', attrs={'class':'rsl-item-nocover rsl-item-nocover-title'})
    date = soup.findAll('p', attrs={'class':'search-item-enterdate'})
    info = soup.find_all('span', attrs={'class': 'js-item-maininfo'})

    name_clean = [i.get_text() for i in name]
    date_clean = [i.get_text() for i in date]
    info_clean = [i.get_text() for i in info]

    diss_dict = {"name": name_clean, "date_submitted": date_clean, 'description': info_clean}
    diss_data = pd.DataFrame(diss_dict)
    diss_data_merged = diss_data_merged.append(diss_data)
    diss_data_merged = pd.DataFrame(diss_data_merged)
    diss_data_merged.to_csv('b1.csv', index=False)
    if len(diss_data_merged) % 500 == 0:
      print(len(diss_data_merged))

for i in range(2, 101):
    br = webdriver.Chrome('chromedriver',chrome_options=chrome_options) 
    br.get('https://search.rsl.ru/#yf=2012&yt=2012&s=default&vc=22.00.00&vc=23.00.00&vc=24.00.00&vc=17.00.00&p='+str(i)+'&d=xdis')
    sleep(3)
    soup = BeautifulSoup(br.page_source)
    br.quit()
    name = soup.findAll('div', attrs={'class':'rsl-item-nocover rsl-item-nocover-title'})
    date = soup.findAll('p', attrs={'class':'search-item-enterdate'})
    info = soup.find_all('span', attrs={'class': 'js-item-maininfo'})

    name_clean = [i.get_text() for i in name]
    date_clean = [i.get_text() for i in date]
    info_clean = [i.get_text() for i in info]

    diss_dict = {"name": name_clean, "date_submitted": date_clean, 'description': info_clean}
    diss_data = pd.DataFrame(diss_dict)
    diss_data_merged = diss_data_merged.append(diss_data)
    diss_data_merged = pd.DataFrame(diss_data_merged)
    diss_data_merged.to_csv('b1.csv', index=False)
    if len(diss_data_merged) % 500 == 0:
      print(len(diss_data_merged))

for i in range(2, 101):
    br = webdriver.Chrome('chromedriver',chrome_options=chrome_options) 
    br.get('https://search.rsl.ru/#yf=2013&yt=2013&s=default&vc=22.00.00&vc=23.00.00&vc=24.00.00&vc=17.00.00&p='+str(i)+'&d=xdis')
    sleep(3)
    soup = BeautifulSoup(br.page_source)
    br.quit()
    name = soup.findAll('div', attrs={'class':'rsl-item-nocover rsl-item-nocover-title'})
    date = soup.findAll('p', attrs={'class':'search-item-enterdate'})
    info = soup.find_all('span', attrs={'class': 'js-item-maininfo'})

    name_clean = [i.get_text() for i in name]
    date_clean = [i.get_text() for i in date]
    info_clean = [i.get_text() for i in info]

    diss_dict = {"name": name_clean, "date_submitted": date_clean, 'description': info_clean}
    diss_data = pd.DataFrame(diss_dict)
    diss_data_merged = diss_data_merged.append(diss_data)
    diss_data_merged = pd.DataFrame(diss_data_merged)
    diss_data_merged.to_csv('b1.csv', index=False)
    if len(diss_data_merged) % 500 == 0:
      print(len(diss_data_merged))

diss_data_merged= pd.DataFrame(columns= ["name", "date_submitted", 'description'])

for i in range(2, 101):
    br = webdriver.Chrome('chromedriver',chrome_options=chrome_options) 
    br.get('https://search.rsl.ru/#yf=2014&yt=2014&s=default&vc=22.00.00&vc=23.00.00&vc=24.00.00&vc=17.00.00&p='+str(i)+'&d=xdis')
    sleep(3)
    soup = BeautifulSoup(br.page_source)
    br.quit()
    name = soup.findAll('div', attrs={'class':'rsl-item-nocover rsl-item-nocover-title'})
    date = soup.findAll('p', attrs={'class':'search-item-enterdate'})
    info = soup.find_all('span', attrs={'class': 'js-item-maininfo'})

    name_clean = [i.get_text() for i in name]
    date_clean = [i.get_text() for i in date]
    info_clean = [i.get_text() for i in info]

    diss_dict = {"name": name_clean, "date_submitted": date_clean, 'description': info_clean}
    diss_data = pd.DataFrame(diss_dict)
    diss_data_merged = diss_data_merged.append(diss_data)
    diss_data_merged = pd.DataFrame(diss_data_merged)
    diss_data_merged.to_csv('b2.csv', index=False)
    if len(diss_data_merged) % 500 == 0:
      print(len(diss_data_merged))

diss_data_merged= pd.DataFrame(columns= ["name", "date_submitted", 'description'])
for i in range(2, 101):
    br = webdriver.Chrome('chromedriver',chrome_options=chrome_options) 
    br.get('https://search.rsl.ru/#yf=2015&yt=2015&s=default&vc=22.00.00&vc=23.00.00&vc=24.00.00&vc=17.00.00&p='+str(i)+'&d=xdis')
    sleep(3)
    soup = BeautifulSoup(br.page_source)
    br.quit()
    name = soup.findAll('div', attrs={'class':'rsl-item-nocover rsl-item-nocover-title'})
    date = soup.findAll('p', attrs={'class':'search-item-enterdate'})
    info = soup.find_all('span', attrs={'class': 'js-item-maininfo'})

    name_clean = [i.get_text() for i in name]
    date_clean = [i.get_text() for i in date]
    info_clean = [i.get_text() for i in info]

    diss_dict = {"name": name_clean, "date_submitted": date_clean, 'description': info_clean}
    diss_data = pd.DataFrame(diss_dict)
    diss_data_merged = diss_data_merged.append(diss_data)
    diss_data_merged = pd.DataFrame(diss_data_merged)
    diss_data_merged.to_csv('b.csv', index=False)
    if len(diss_data_merged) % 500 == 0:
      print(len(diss_data_merged))

for i in range(2, 101):
    br = webdriver.Chrome('chromedriver',chrome_options=chrome_options) 
    br.get('https://search.rsl.ru/#yf=2016&yt=2016&s=default&vc=22.00.00&vc=23.00.00&vc=24.00.00&vc=17.00.00&p='+str(i)+'&d=xdis')
    sleep(3)
    soup = BeautifulSoup(br.page_source)
    br.quit()
    name = soup.findAll('div', attrs={'class':'rsl-item-nocover rsl-item-nocover-title'})
    date = soup.findAll('p', attrs={'class':'search-item-enterdate'})
    info = soup.find_all('span', attrs={'class': 'js-item-maininfo'})

    name_clean = [i.get_text() for i in name]
    date_clean = [i.get_text() for i in date]
    info_clean = [i.get_text() for i in info]

    diss_dict = {"name": name_clean, "date_submitted": date_clean, 'description': info_clean}
    diss_data = pd.DataFrame(diss_dict)
    diss_data_merged = diss_data_merged.append(diss_data)
    diss_data_merged = pd.DataFrame(diss_data_merged)
    diss_data_merged.to_csv('b.csv', index=False)
    if len(diss_data_merged) % 500 == 0:
      print(len(diss_data_merged))

for i in range(2, 101):
    br = webdriver.Chrome('chromedriver',chrome_options=chrome_options) 
    br.get('https://search.rsl.ru/#yf=2017&yt=2017&s=default&vc=22.00.00&vc=23.00.00&vc=24.00.00&vc=17.00.00&p='+str(i)+'&d=xdis')
    sleep(3)
    soup = BeautifulSoup(br.page_source)
    br.quit()
    name = soup.findAll('div', attrs={'class':'rsl-item-nocover rsl-item-nocover-title'})
    date = soup.findAll('p', attrs={'class':'search-item-enterdate'})
    info = soup.find_all('span', attrs={'class': 'js-item-maininfo'})

    name_clean = [i.get_text() for i in name]
    date_clean = [i.get_text() for i in date]
    info_clean = [i.get_text() for i in info]

    diss_dict = {"name": name_clean, "date_submitted": date_clean, 'description': info_clean}
    diss_data = pd.DataFrame(diss_dict)
    diss_data_merged = diss_data_merged.append(diss_data)
    diss_data_merged = pd.DataFrame(diss_data_merged)
    diss_data_merged.to_csv('b.csv', index=False)
    if len(diss_data_merged) % 500 == 0:
      print(len(diss_data_merged))

for i in range(2, 101):
    br = webdriver.Chrome('chromedriver',chrome_options=chrome_options) 
    br.get('https://search.rsl.ru/#yf=2018&yt=2018&s=default&vc=22.00.00&vc=23.00.00&vc=24.00.00&vc=17.00.00&p='+str(i)+'&d=xdis')
    sleep(3)
    soup = BeautifulSoup(br.page_source)
    br.quit()
    name = soup.findAll('div', attrs={'class':'rsl-item-nocover rsl-item-nocover-title'})
    date = soup.findAll('p', attrs={'class':'search-item-enterdate'})
    info = soup.find_all('span', attrs={'class': 'js-item-maininfo'})

    name_clean = [i.get_text() for i in name]
    date_clean = [i.get_text() for i in date]
    info_clean = [i.get_text() for i in info]

    diss_dict = {"name": name_clean, "date_submitted": date_clean, 'description': info_clean}
    diss_data = pd.DataFrame(diss_dict)
    diss_data_merged = diss_data_merged.append(diss_data)
    diss_data_merged = pd.DataFrame(diss_data_merged)
    diss_data_merged.to_csv('b.csv', index=False)
    if len(diss_data_merged) % 500 == 0:
      print(len(diss_data_merged))

from google.colab import drive
drive.mount('/content/drive')

for i in range(2, 101):
    br = webdriver.Chrome('chromedriver',chrome_options=chrome_options) 
    br.get('https://search.rsl.ru/#yf=2019&yt=2019&s=default&vc=22.00.00&vc=23.00.00&vc=24.00.00&vc=17.00.00&p='+str(i)+'&d=xdis')
    sleep(3)
    soup = BeautifulSoup(br.page_source)
    br.quit()
    name = soup.findAll('div', attrs={'class':'rsl-item-nocover rsl-item-nocover-title'})
    date = soup.findAll('p', attrs={'class':'search-item-enterdate'})
    info = soup.find_all('span', attrs={'class': 'js-item-maininfo'})

    name_clean = [i.get_text() for i in name]
    date_clean = [i.get_text() for i in date]
    info_clean = [i.get_text() for i in info]

    diss_dict = {"name": name_clean, "date_submitted": date_clean, 'description': info_clean}
    diss_data = pd.DataFrame(diss_dict)
    diss_data_merged = diss_data_merged.append(diss_data)
    diss_data_merged = pd.DataFrame(diss_data_merged)
    diss_data_merged.to_csv('b.csv', index=False)
    if len(diss_data_merged) % 500 == 0:
      print(len(diss_data_merged))

for i in range(2, 101):
    br = webdriver.Chrome('chromedriver',chrome_options=chrome_options) 
    br.get('https://search.rsl.ru/#yf=2020&yt=2020&s=default&vc=22.00.00&vc=23.00.00&vc=24.00.00&vc=17.00.00&p='+str(i)+'&d=xdis')
    sleep(3)
    soup = BeautifulSoup(br.page_source)
    br.quit()
    name = soup.findAll('div', attrs={'class':'rsl-item-nocover rsl-item-nocover-title'})
    date = soup.findAll('p', attrs={'class':'search-item-enterdate'})
    info = soup.find_all('span', attrs={'class': 'js-item-maininfo'})

    name_clean = [i.get_text() for i in name]
    date_clean = [i.get_text() for i in date]
    info_clean = [i.get_text() for i in info]

    diss_dict = {"name": name_clean, "date_submitted": date_clean, 'description': info_clean}
    diss_data = pd.DataFrame(diss_dict)
    diss_data_merged = diss_data_merged.append(diss_data)
    diss_data_merged = pd.DataFrame(diss_data_merged)
    diss_data_merged.to_csv('b1.csv', index=False)
    if len(diss_data_merged) % 500 == 0:
      print(len(diss_data_merged))

"""Психологи философия



"""

for y in range(2010, 2021):
  for i in range(2, 101):
      br = webdriver.Chrome('chromedriver',chrome_options=chrome_options) 
      br.get('https://search.rsl.ru/#yf='+str(y)+'&yt='+str(y)+'&s=default&vc=09.00.00&vc=19.00.00&p='+str(i)+'&d=xdis')
      sleep(3)
      soup = BeautifulSoup(br.page_source)
      br.quit()
      name = soup.findAll('div', attrs={'class':'rsl-item-nocover rsl-item-nocover-title'})
      date = soup.findAll('p', attrs={'class':'search-item-enterdate'})
      info = soup.find_all('span', attrs={'class': 'js-item-maininfo'})

      name_clean = [i.get_text() for i in name]
      date_clean = [i.get_text() for i in date]
      info_clean = [i.get_text() for i in info]

      diss_dict = {"name": name_clean, "date_submitted": date_clean, 'description': info_clean}
      diss_data = pd.DataFrame(diss_dict)
      diss_data_merged = diss_data_merged.append(diss_data)
      diss_data_merged = pd.DataFrame(diss_data_merged)
      diss_data_merged.to_csv('b1.csv', index=False)
      if len(diss_data_merged) % 500 == 0:
        print(len(diss_data_merged))

"""Физика"""

diss_data_merged= pd.DataFrame(columns= ["name", "date_submitted", 'description'])

for y in range(2010, 2021):
  for i in range(2, 101):
      br = webdriver.Chrome('chromedriver',chrome_options=chrome_options) 
      br.get('https://search.rsl.ru/#yf='+str(y)+'&yt='+str(y)+'&s=default&vc=01.04.00&p='+str(i)+'&d=xdis')
      sleep(3)
      soup = BeautifulSoup(br.page_source)
      br.quit()
      name = soup.findAll('div', attrs={'class':'rsl-item-nocover rsl-item-nocover-title'})
      date = soup.findAll('p', attrs={'class':'search-item-enterdate'})
      info = soup.find_all('span', attrs={'class': 'js-item-maininfo'})

      name_clean = [i.get_text() for i in name]
      date_clean = [i.get_text() for i in date]
      info_clean = [i.get_text() for i in info]

      diss_dict = {"name": name_clean, "date_submitted": date_clean, 'description': info_clean}
      diss_data = pd.DataFrame(diss_dict)
      diss_data_merged = diss_data_merged.append(diss_data)
      diss_data_merged = pd.DataFrame(diss_data_merged)
      diss_data_merged.to_csv('phys1.csv', index=False)
      if len(diss_data_merged) % 500 == 0:
        print(len(diss_data_merged))

for y in range(2010, 2021):
  for i in range(2, 101):
      br = webdriver.Chrome('chromedriver',chrome_options=chrome_options) 
      br.get('https://search.rsl.ru/#yf='+str(y)+'&yt='+str(y)+'&s=default&vc=01.01.00&p='+str(i)+'&d=xdis')
      sleep(3)
      soup = BeautifulSoup(br.page_source)
      br.quit()
      name = soup.findAll('div', attrs={'class':'rsl-item-nocover rsl-item-nocover-title'})
      date = soup.findAll('p', attrs={'class':'search-item-enterdate'})
      info = soup.find_all('span', attrs={'class': 'js-item-maininfo'})

      name_clean = [i.get_text() for i in name]
      date_clean = [i.get_text() for i in date]
      info_clean = [i.get_text() for i in info]

      diss_dict = {"name": name_clean, "date_submitted": date_clean, 'description': info_clean}
      diss_data = pd.DataFrame(diss_dict)
      diss_data_merged = diss_data_merged.append(diss_data)
      diss_data_merged = pd.DataFrame(diss_data_merged)
      diss_data_merged.to_csv('b2.csv', index=False)
      if len(diss_data_merged) % 500 == 0:
        print(len(diss_data_merged))

for y in range(2010, 2021):
  for i in range(2, 101):
      br = webdriver.Chrome('chromedriver',chrome_options=chrome_options) 
      br.get('https://search.rsl.ru/#yf='+str(y)+'&yt='+str(y)+'&s=default&vc=01.02.00&p='+str(i)+'&d=xdis')
      sleep(3)
      soup = BeautifulSoup(br.page_source)
      br.quit()
      name = soup.findAll('div', attrs={'class':'rsl-item-nocover rsl-item-nocover-title'})
      date = soup.findAll('p', attrs={'class':'search-item-enterdate'})
      info = soup.find_all('span', attrs={'class': 'js-item-maininfo'})

      name_clean = [i.get_text() for i in name]
      date_clean = [i.get_text() for i in date]
      info_clean = [i.get_text() for i in info]

      diss_dict = {"name": name_clean, "date_submitted": date_clean, 'description': info_clean}
      diss_data = pd.DataFrame(diss_dict)
      diss_data_merged = diss_data_merged.append(diss_data)
      diss_data_merged = pd.DataFrame(diss_data_merged)
      diss_data_merged.to_csv('b2.csv', index=False)
      if len(diss_data_merged) % 500 == 0:
        print(len(diss_data_merged))

for y in range(2010, 2021):
  for i in range(2, 101):
      br = webdriver.Chrome('chromedriver',chrome_options=chrome_options) 
      br.get('https://search.rsl.ru/#yf='+str(y)+'&yt='+str(y)+'&s=default&vc=01.03.00&p='+str(i)+'&d=xdis')
      sleep(3)
      soup = BeautifulSoup(br.page_source)
      br.quit()
      name = soup.findAll('div', attrs={'class':'rsl-item-nocover rsl-item-nocover-title'})
      date = soup.findAll('p', attrs={'class':'search-item-enterdate'})
      info = soup.find_all('span', attrs={'class': 'js-item-maininfo'})

      name_clean = [i.get_text() for i in name]
      date_clean = [i.get_text() for i in date]
      info_clean = [i.get_text() for i in info]

      diss_dict = {"name": name_clean, "date_submitted": date_clean, 'description': info_clean}
      diss_data = pd.DataFrame(diss_dict)
      diss_data_merged = diss_data_merged.append(diss_data)
      diss_data_merged = pd.DataFrame(diss_data_merged)
      diss_data_merged.to_csv('b2.csv', index=False)
      if len(diss_data_merged) % 500 == 0:
        print(len(diss_data_merged))